{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-hygiene",
   "metadata": {},
   "source": [
    "Scripts for converting isanlp.DiscourseUnit RST representation to ``dummy_format_data`` with the following keys:\n",
    "- InputDocs : list of lists with plain tokens of each document\n",
    "- EduBreak_TokenLevel : list of lists with the token positions of right EDU ends of each document\n",
    "- SentBreak : self-describing\n",
    "- Docs_structure : list of lists with the descriptions of binary relations in the document tree in a format such as ``(2:Satellite=Cause:2,3:Nucleus=Cause)  # EDU on left, EDU on right\n",
    "(1:Nucleus=span:1,2:Satellite=Elaboration:3)  # one is EDU, other is not\n",
    "(1:Nucleus=Sequence:3,4:Nucleus=Sequence:6)  # nether of them is EDU``\n",
    "\n",
    "We need two data directories for this notebook:\n",
    "- ``corpus_du/`` containing pickled RST corpus in isanlp.DiscourseUnit format (*.du)\n",
    "- ``annots/`` containing pickled isanlp annotation (texts, tokens, sentences, etc.) (*.annot.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_docs(doc_annot: dict):\n",
    "    \"\"\" InputDocs : list of lists with plain tokens of each document \"\"\"\n",
    "    return [token.text for token in doc_annot.get('tokens')]\n",
    "\n",
    "def get_edu_breaks(doc_trees: list, doc_annot: dict):\n",
    "    \"\"\" EduBreak_TokenLevel : list of lists with the token positions of right EDU ends of each document \"\"\"\n",
    "    \n",
    "    def extr_edus(tree):\n",
    "        if tree.relation == 'elementary':\n",
    "            return [(tree.start, tree.end)]\n",
    "        else:\n",
    "            tt = []\n",
    "            tt += extr_edus(tree.left)\n",
    "            tt += extr_edus(tree.right)\n",
    "        return tt\n",
    "    \n",
    "    def map_offset_to_tokens(offset):\n",
    "        begin, end = -1, -1\n",
    "        for i, token in enumerate(doc_annot['tokens']):\n",
    "            if begin == -1 and token.begin > offset[0]:\n",
    "                begin = i - 1\n",
    "            if begin != -1:\n",
    "                if token.end > offset[1]:\n",
    "                    end = i - 1\n",
    "                    return begin, end\n",
    "        return begin, i-1\n",
    "    \n",
    "    edus = []\n",
    "    for tree in doc_trees:\n",
    "        edus += extr_edus(tree)\n",
    "    \n",
    "    return [map_offset_to_tokens(offset)[1] for offset in edus]\n",
    "\n",
    "def get_sentence_breaks(doc_annot: dict):\n",
    "    \"\"\" SentBreak for sentence breaks in terms of token offsets \"\"\"\n",
    "    return [sentence.end - 1 for sentence in annot.get('sentences')]\n",
    "\n",
    "def leftmostid(tree):\n",
    "    if tree.left:\n",
    "        return leftmostid(tree.left)\n",
    "    return tree.id\n",
    "\n",
    "def rightmostid(tree):\n",
    "    if tree.right:\n",
    "        return rightmostid(tree.right)\n",
    "    return tree.id\n",
    "\n",
    "def du_to_docs_structure(tree: dict, du_counter: int):    \n",
    "    if tree.relation != 'elementary':\n",
    "        left_nuclearity = 'Satellite' if tree.nuclearity == 'SN' else 'Nucleus'\n",
    "        right_nuclearity = 'Satellite' if tree.nuclearity == 'NS' else 'Nucleus'\n",
    "        left_relation = tree.relation\n",
    "        right_relation = tree.relation\n",
    "        \n",
    "        left_id_1 = leftmostid(tree.left) + du_counter\n",
    "        left_id_2 = rightmostid(tree.left) + du_counter\n",
    "        right_id_1 = leftmostid(tree.right) + du_counter\n",
    "        right_id_2 = rightmostid(tree.right) + du_counter\n",
    "            \n",
    "        if left_nuclearity == 'Satellite':\n",
    "            right_relation = 'span'\n",
    "        \n",
    "        if right_nuclearity == 'Satellite':\n",
    "            left_relation = 'span'\n",
    "            \n",
    "        relstring_l = f'{left_id_1}:{left_nuclearity}={left_relation}:{left_id_2}'\n",
    "        relstring_r = f'{right_id_1}:{right_nuclearity}={right_relation}:{right_id_2}'\n",
    "\n",
    "        left_subtree_struct = du_to_docs_structure(tree.left, du_counter) or []\n",
    "        right_subtree_struct = du_to_docs_structure(tree.right, du_counter) or []\n",
    "        return [f'({relstring_l},{relstring_r})'] + left_subtree_struct + right_subtree_struct\n",
    "    \n",
    "    \n",
    "def collect_edus(docs_structure):\n",
    "    edus_id = []\n",
    "    for entry in docs_structure:\n",
    "        left, right = entry.split(',')\n",
    "        left = left.replace('(', '').split(':')\n",
    "        du1, du2 = left[0], left[2]\n",
    "        if du1 == du2:\n",
    "            edus_id.append(int(du1))\n",
    "        \n",
    "        right = right.replace(')', '').split(':')\n",
    "        du1, du2 = right[0], right[2]\n",
    "        if du1 == du2:\n",
    "            edus_id.append(int(du1))\n",
    "    return edus_id\n",
    "    \n",
    "def get_docs_structure(doc_trees: list):\n",
    "    result = []\n",
    "    du_counter = 0\n",
    "    for tree in doc_trees:\n",
    "        structure = du_to_docs_structure(tree, du_counter)\n",
    "        if structure:\n",
    "            result += structure\n",
    "            du_counter += len(structure)\n",
    "        else:\n",
    "            du_counter += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = ['news2_47', 'blogs_0', 'news2_14', 'news1_39', 'blogs_14',\n",
    "       'blogs_27', 'news2_41', 'blogs_45', 'news1_70', 'blogs_71',\n",
    "       'blogs_84', 'blogs_8', 'blogs_16', 'blogs_92', 'blogs_28',\n",
    "       'news2_26', 'blogs_4', 'blogs_46', 'blogs_44', 'news1_41',\n",
    "       'news1_21', 'news2_29', 'news1_33', 'blogs_48', 'blogs_88',\n",
    "       'news1_18', 'blogs_43', 'news2_22', 'blogs_96', 'news2_18',\n",
    "       'news2_39', 'news2_42', 'blogs_97', 'news2_31', 'news2_2',\n",
    "       'blogs_23', 'blogs_82', 'news2_36', 'blogs_18', 'news2_28',\n",
    "       'blogs_90', 'blogs_51', 'blogs_36', 'news1_49', 'blogs_98',\n",
    "       'news1_51', 'news2_12', 'news1_54', 'blogs_73', 'blogs_50',\n",
    "       'news2_9', 'blogs_3', 'blogs_34', 'blogs_32', 'blogs_13',\n",
    "       'news1_45', 'news1_13', 'news1_10', 'blogs_62', 'blogs_66',\n",
    "       'blogs_74', 'blogs_29', 'blogs_77', 'blogs_65', 'blogs_53',\n",
    "       'blogs_55', 'blogs_7', 'blogs_67', 'news2_40', 'news2_46',\n",
    "       'news1_35', 'blogs_95', 'news2_32', 'news1_72', 'news1_9',\n",
    "       'blogs_93', 'blogs_58', 'news1_17', 'news2_27', 'news1_58',\n",
    "       'news2_24', 'news1_46', 'blogs_37', 'blogs_25', 'blogs_81',\n",
    "       'news1_38', 'blogs_35', 'blogs_59', 'blogs_2', 'news1_56',\n",
    "       'blogs_24', 'blogs_94', 'news2_45', 'blogs_75', 'news1_14',\n",
    "       'news2_25', 'blogs_11', 'blogs_80', 'blogs_40', 'news1_52',\n",
    "       'news1_32', 'news2_33', 'news1_71', 'blogs_12', 'blogs_38',\n",
    "       'blogs_70', 'news2_5', 'news2_20', 'news2_15', 'blogs_87',\n",
    "       'blogs_56', 'blogs_78', 'blogs_91', 'news1_4', 'news1_3',\n",
    "       'blogs_85', 'news1_62', 'blogs_68', 'blogs_47', 'news1_26',\n",
    "       'blogs_6', 'news1_34', 'blogs_41', 'blogs_42', 'news1_30',\n",
    "       'news1_61', 'news2_17', 'news1_55', 'news1_48', 'news2_37',\n",
    "       'news1_69', 'news1_7', 'news2_7', 'news1_63', 'news1_73',\n",
    "       'news2_3', 'blogs_102', 'news1_53', 'news2_19', 'news1_43',\n",
    "       'blogs_101', 'news1_65', 'news1_27', 'news2_6', 'news1_8',\n",
    "       'news1_37', 'blogs_1', 'news1_11', 'news1_64', 'news2_8',\n",
    "       'blogs_61', 'news2_35', 'news1_16', 'blogs_89', 'news1_67',\n",
    "       'news1_1', 'blogs_79', 'news1_31', 'news1_66', 'news1_5',\n",
    "       'news2_10', 'news1_12', 'news1_74', 'news1_68', 'news1_15',\n",
    "       'news1_75', 'news2_0', 'news1_2']\n",
    "\n",
    "dev_filenames = ['blogs_26', 'blogs_9', 'blogs_19', 'news2_43', 'blogs_15',\n",
    "       'blogs_54', 'news1_20', 'news1_22', 'blogs_22', 'news1_44',\n",
    "       'blogs_33', 'blogs_100', 'blogs_103', 'blogs_10', 'blogs_20',\n",
    "       'news1_60', 'news2_44', 'news1_36', 'news1_59', 'news2_23',\n",
    "       'news2_30', 'blogs_30', 'news2_11', 'news1_6', 'blogs_49',\n",
    "       'news1_50', 'news2_1', 'news1_79', 'news2_13', 'blogs_64',\n",
    "       'news1_76', 'blogs_83', 'news2_49', 'blogs_57', 'news2_21',\n",
    "       'blogs_5', 'blogs_76', 'news1_19', 'news1_40', 'news1_57']\n",
    "\n",
    "test_filenames = ['blogs_21', 'news2_34', 'blogs_52', 'blogs_17', 'news1_47',\n",
    "       'blogs_99', 'blogs_72', 'blogs_60', 'news1_25', 'news1_28',\n",
    "       'blogs_63', 'news2_38', 'news2_48', 'blogs_69', 'blogs_39',\n",
    "       'blogs_31', 'news1_23', 'blogs_86', 'news1_78', 'news2_4',\n",
    "       'news1_77', 'news1_24', 'news1_29', 'news1_42', 'news2_16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp import PipelineCommon\n",
    "from isanlp.processor_remote import ProcessorRemote\n",
    "from isanlp.ru.converter_mystem_to_ud import ConverterMystemToUd\n",
    "from isanlp.ru.processor_mystem import ProcessorMystem\n",
    "from isanlp.processor_razdel import ProcessorRazdel\n",
    "\n",
    "\n",
    "ppl = PipelineCommon([\n",
    "    (ProcessorRazdel(), ['text'],\n",
    "    {'tokens': 'tokens',\n",
    "     'sentences': 'sentences'}),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-gathering",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = {\n",
    "    'InputDocs': [],\n",
    "    'EduBreak_TokenLevel': [],\n",
    "    'SentBreak': [],\n",
    "    'Docs_structure': [],\n",
    "}\n",
    "\n",
    "dev = {\n",
    "    'InputDocs': [],\n",
    "    'EduBreak_TokenLevel': [],\n",
    "    'SentBreak': [],\n",
    "    'Docs_structure': [],\n",
    "}\n",
    "\n",
    "test = {\n",
    "    'InputDocs': [],\n",
    "    'EduBreak_TokenLevel': [],\n",
    "    'SentBreak': [],\n",
    "    'Docs_structure': [],\n",
    "}\n",
    "\n",
    "for file in tqdm(glob.glob('corpus_du/*.du')):\n",
    "#     annot_file = file.replace('corpus_du/', 'annots/').split('_part_')[0] + '.annot.pkl'\n",
    "#     annot = pickle.load(open(annot_file, 'rb'))\n",
    "    \n",
    "    trees = [pickle.load(open(file, 'rb'))]\n",
    "    annot = ppl(trees[0].text)\n",
    "#     for part in sorted(glob.glob(file.replace('part_0.du', 'part_*.du')),\n",
    "#                        key=lambda x: float(re.findall(\"(\\d+)\",x)[-1])):\n",
    "#         with open(part, 'rb') as f:\n",
    "#             trees.append(pickle.load(f))\n",
    "    \n",
    "    edus = get_edu_breaks(trees, annot)\n",
    "    if len(edus) > 1:\n",
    "        clear_filename = file.replace('corpus_du/', '').split('_part_')[0]\n",
    "        if clear_filename in train_filenames:    \n",
    "            train['InputDocs'].append(get_input_docs(annot))\n",
    "            train['EduBreak_TokenLevel'].append(get_edu_breaks(trees, annot))\n",
    "            train['SentBreak'].append(get_sentence_breaks(annot))\n",
    "            train['Docs_structure'].append(get_docs_structure(trees))\n",
    "\n",
    "        elif clear_filename in dev_filenames:\n",
    "            dev['InputDocs'].append(get_input_docs(annot))\n",
    "            dev['EduBreak_TokenLevel'].append(get_edu_breaks(trees, annot))\n",
    "            dev['SentBreak'].append(get_sentence_breaks(annot))\n",
    "            dev['Docs_structure'].append(get_docs_structure(trees))\n",
    "\n",
    "        elif clear_filename in test_filenames:\n",
    "            test['InputDocs'].append(get_input_docs(annot))\n",
    "            test['EduBreak_TokenLevel'].append(get_edu_breaks(trees, annot))\n",
    "            test['SentBreak'].append(get_sentence_breaks(annot))\n",
    "            test['Docs_structure'].append(get_docs_structure(trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train['InputDocs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-greene",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(dev['InputDocs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test['Docs_structure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-effort",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['InputDocs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = train['Docs_structure'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "for i, struct in enumerate(train['Docs_structure']):\n",
    "    if not struct:\n",
    "#         print(train['InputDocs'][i])\n",
    "        print(train['EduBreak_TokenLevel'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_edus(docs_structure):\n",
    "    edus_id = []\n",
    "    for entry in docs_structure:\n",
    "        left, right = entry.split(',')\n",
    "        left = left.replace('(', '').split(':')\n",
    "        du1, du2 = left[0], left[2]\n",
    "        if du1 == du2:\n",
    "            edus_id.append(int(du1))\n",
    "        \n",
    "        right = right.replace(')', '').split(':')\n",
    "        du1, du2 = right[0], right[2]\n",
    "        if du1 == du2:\n",
    "            edus_id.append(int(du1))\n",
    "    return edus_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-benefit",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max(collect_edus(dd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../processed_data'\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "with open(os.path.join(output_dir, 'train_approach1'), 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "    \n",
    "with open(os.path.join(output_dir, 'dev_approach1'), 'wb') as f:\n",
    "    pickle.dump(dev, f)\n",
    "    \n",
    "with open(os.path.join(output_dir, 'test_approach1'), 'wb') as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec ../data/model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat ../data/model.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -2 ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget http://vectors.nlpl.eu/repository/20/213.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip 213.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = np.load('model.model.vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.load('model.model.vectors_vocab.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-vault",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_text = KeyedVectors.load('model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/model.txt', 'w') as f:\n",
    "    f.write('header\\n')\n",
    "    for key in tqdm(list(wv_from_text.vocab.keys())):\n",
    "        f.write(key)\n",
    "        f.write('\\t')\n",
    "        f.write(' '.join(map(str, wv_from_text.get_vector(key))))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-freeze",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! cat ../data/model.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class Embedding(object):\n",
    "\n",
    "    def __init__(self, tokens, vectors, unk=None):\n",
    "        self.tokens = tokens\n",
    "        self.vectors = torch.tensor(vectors)\n",
    "        self.pretrained = {w: v for w, v in zip(tokens, vectors)}\n",
    "        self.unk = unk\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __contains__(self, token):\n",
    "        return token in self.pretrained\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.vectors.size(1)\n",
    "\n",
    "    @property\n",
    "    def unk_index(self):\n",
    "        if self.unk is not None:\n",
    "            return self.tokens.index(self.unk)\n",
    "        else:\n",
    "            raise AttributeError\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path, unk=None):\n",
    "        with open(path, 'r') as f:\n",
    "            lines = [line for line in f][1:]\n",
    "        splits = [line.split() for line in lines]\n",
    "        tokens, vectors = zip(*[(s[0], list(map(float, s[1:])))\n",
    "                                for s in splits])\n",
    "\n",
    "        return cls(tokens, vectors, unk=unk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Embedding.load('../data/model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-cement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tok in emb.tokens:\n",
    "    if 'unk' in tok:\n",
    "        print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-evanescence",
   "metadata": {},
   "outputs": [],
   "source": [
    "'unknown' in emb.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.tokens[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
